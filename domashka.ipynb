{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ДЗ_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: click in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xoxol\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\xoxol\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\xoxol\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xoxol\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\xoxol\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python.exe -m pip install requests beautifulsoup4 nltk wordcloud matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xoxol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xoxol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xoxol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\xoxol/nltk_data', 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data', 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data', 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\xoxol\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'path/to/your/nltk_data', 'path/to/your/nltk_data', 'path/to/your/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path.append(\"path/to/your/nltk_data\")\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собрано 200 ссылок на статьи.\n"
     ]
    }
   ],
   "source": [
    "def get_article_links(page_count=2):\n",
    "    links = []\n",
    "    for page in range(1, page_count + 1):\n",
    "        url = f'https://3dnews.ru/news/page-{page}.html'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "        for article in soup.find_all('a', class_='entry-header'):\n",
    "            link = article['href']\n",
    "            if link.startswith('/'):\n",
    "                link = 'https://3dnews.ru' + link\n",
    "            links.append(link)\n",
    "    return links\n",
    "\n",
    "\n",
    "article_links = get_article_links(page_count=2)\n",
    "print(f'Собрано {len(article_links)} ссылок на статьи.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собрано данных для 200 статей.\n"
     ]
    }
   ],
   "source": [
    "def parse_article(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    title = soup.find('h1').get_text() if soup.find('h1') else 'N/A'\n",
    "    date = soup.find('span', class_='article_date').get_text() if soup.find('span', class_='article_date') else 'N/A'\n",
    "    views = soup.find('div', class_='views_count').get_text() if soup.find('div', class_='views_count') else 'N/A'\n",
    "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "\n",
    "    return {'title': title, 'date': date, 'views': views, 'text': text}\n",
    "\n",
    "\n",
    "data = [parse_article(link) for link in article_links]\n",
    "print(f'Собрано данных для {len(data)} статей.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\xoxol/nltk_data'\n    - 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\xoxol\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'path/to/your/nltk_data'\n    - 'path/to/your/nltk_data'\n    - 'path/to/your/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mThis is a sample text for preprocessing.\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[0;32m     12\u001b[0m     {\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mAnother example text to clean.\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m data:\n\u001b[1;32m---> 16\u001b[0m     article[\u001b[39m'\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m preprocess_text(article[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mОчистка данных завершена.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n\u001b[1;32m----> 6\u001b[0m     words \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      7\u001b[0m     words \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(w\u001b[39m.\u001b[39mlower()) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39misalnum() \u001b[39mand\u001b[39;00m w\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(words)\n",
      "File \u001b[1;32mc:\\Users\\xoxol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\xoxol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\xoxol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\xoxol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\xoxol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\xoxol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\xoxol/nltk_data'\n    - 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\xoxol\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\xoxol\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'path/to/your/nltk_data'\n    - 'path/to/your/nltk_data'\n    - 'path/to/your/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words if w.isalnum() and w.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "data = [\n",
    "    {'text': 'This is a sample text for preprocessing.'},\n",
    "    {'text': 'Another example text to clean.'}\n",
    "]\n",
    "\n",
    "for article in data:\n",
    "    article['cleaned_text'] = preprocess_text(article['text'])\n",
    "\n",
    "print(\"Очистка данных завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cleaned_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(article[\u001b[39m'\u001b[39;49m\u001b[39mcleaned_text\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m article \u001b[39min\u001b[39;49;00m data)\n\u001b[0;32m      2\u001b[0m word_count \u001b[39m=\u001b[39m Counter(all_text\u001b[39m.\u001b[39msplit())\n\u001b[0;32m      5\u001b[0m common_words \u001b[39m=\u001b[39m word_count\u001b[39m.\u001b[39mmost_common(\u001b[39m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(article[\u001b[39m'\u001b[39;49m\u001b[39mcleaned_text\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m data)\n\u001b[0;32m      2\u001b[0m word_count \u001b[39m=\u001b[39m Counter(all_text\u001b[39m.\u001b[39msplit())\n\u001b[0;32m      5\u001b[0m common_words \u001b[39m=\u001b[39m word_count\u001b[39m.\u001b[39mmost_common(\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cleaned_text'"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(article['cleaned_text'] for article in data)\n",
    "word_count = Counter(all_text.split())\n",
    "\n",
    "\n",
    "common_words = word_count.most_common(10)\n",
    "words, counts = zip(*common_words)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, counts)\n",
    "plt.title('10 самых частотных слов')\n",
    "plt.xlabel('Слова')\n",
    "plt.ylabel('Частота')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
